{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cemac/LIFD_GenerativeAdversarialNetworks/blob/main/GANS_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKGKRqNgBjvk"
      },
      "source": [
        "<div style=\"background-color: #ccffcc; padding: 10px;\">\n",
        "    <h1> Tutorial 5 </h1>\n",
        "    <h2> Introduction to Generative Adversarial Networks </h2>\n",
        "</div>      \n",
        "\n",
        "\n",
        "# Overview\n",
        "\n",
        "This Jupyter notebook will walk you through building a simple GAN model and training it on the [MNIST dataset](https://en.wikipedia.org/wiki/MNIST_database) using [PyTorch](https://pytorch.org/). PyTorch is a very powerful library written in Python and C++ that facilitates the building and training of neural networks.\n",
        "\n",
        "This tutorial was adapted from Caitlin Howarth's Presentation and tutorial as part of the LIFD Work shop series. Some of the code and explanations from this tutorial came from [Aleksa Gordić's pyctorch-gans repository](https://github.com/gordicaleksa/pytorch-gans). It contains some more in-depth explanations, as well as examples of more advanced GAN models than we covered here.\n",
        "\n",
        "\n",
        "## The very basics\n",
        "\n",
        "If you know nothing about neural networks there is a [toy neural network python code example](https://github.com/cemac/LIFD_ENV_ML_NOTEBOOKS/tree/main/ToyNeuralNetwork) included in the [LIFD ENV ML Notebooks Repository]( https://github.com/cemac/LIFD_ENV_ML_NOTEBOOKS). Creating a 2 layer neural network to illustrate the fundamentals of how Neural Networks work and the equivlent code using the python machine learning library [keras](https://keras.io/).\n",
        "\n",
        "## Recommended reading\n",
        "\n",
        "\n",
        "* [Introduction to Neural Networks](https://victorzhou.com/blog/intro-to-neural-networks/)\n",
        "* [Introduction to Generative Adversarial Newtworks](https://towardsdatascience.com/fundamentals-of-generative-adversarial-networks-b7ca8c34f0bc)\n",
        "* [Auto Encoders](https://towardsdatascience.com/applied-deep-learning-part-3-autoencoders-1c083af4d798)\n",
        "* [Neural Networks for Regression Problems](https://towardsdatascience.com/deep-neural-networks-for-regression-problems-81321897ca33)\n",
        "* [Digitizing Sketches of the Earth’s Surface with the AI Behind Deep Fakes (IBM Earth Science Example)](https://www.ibm.com/blogs/research/2019/06/good-gans/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5yptSH99asiE"
      },
      "source": [
        "<hr>\n",
        "\n",
        "\n",
        "<div style=\"background-color: #e6ccff; padding: 10px;\">\n",
        "    \n",
        "<h1> Machine Learning Theory </h1>\n",
        "\n",
        "Generative Adversarial Newtworks are a pair of Neural Networks that work to generate rather than predict. The example you've probably see in [mainstream media](https://www.artificialintelligence-news.com/2022/05/09/kendrick-lamar-uses-deepfakes-in-latest-music-video/) is deepfakes, where the network has been trained to produce images of humans that are so good even humans can't tell the difference  \n",
        "    \n",
        "Generative Adversarial Newtworks are made up of two [Neural Networks](): a generator and a discriminator. Both networks are equally bad at their respective tasks initially. As training continues, both networks compete with one another to get better at their respective tasks. Eventually, the generator becomes so good that humans can’t tell the difference (hopefully!)\n",
        "    \n",
        "**The Generator**\n",
        "\n",
        "<a href=\"\">\n",
        "<img src=\"https://github.com/cemac/LIFD_GenerativeAdversarialNetworks/blob/main/figs/generator.png?raw=1\">\n",
        "</a>\n",
        "\n",
        "The generator takes a latent code as an input, and then decodes that vector into an output.\n",
        "\n",
        "Different latent codes should result in different outputs.\n",
        "\n",
        "In the original implementation of GANs, the generator never actually sees the training dataset, and is solely guided by the discriminator.\n",
        "\n",
        "    \n",
        "**The Discriminator**    \n",
        "<a href=\"\">\n",
        "<img src=\"https://github.com/cemac/LIFD_GenerativeAdversarialNetworks/blob/main/figs/discriminator.png?raw=1\">\n",
        "</a>\n",
        "\n",
        "    \n",
        "The discriminator is just a simple regression network and outputs a score from 0 (fake) to 1 (real).\n",
        "\n",
        "Values closer to 0 or 1 represent increased certainty that a sample is fake or real respectively.\n",
        "    \n",
        "**Loss Functions**\n",
        "\n",
        "Adversarial loss is essentially a minimax game, as proposed by Ian Goodfellow et al. [1]\n",
        "\n",
        "The discriminator attempts to maximise the likelihood of correctly predicting real samples as real, and fake samples as fake.\n",
        "\n",
        "The generator attempts to maximise the likelihood of the discriminator predicting that the fake samples it generates are real.\n",
        "\n",
        "* The discriminator loss is defined as: , where  is the discriminator,  is the generator,  is a sample from the real distribution and  is a latent vector given to the generator.\n",
        "\n",
        "* The generator loss is defined as: , where  is the discriminator,  is the generator, and  is a latent vector given to the generator.\n",
        "    \n",
        "References:\n",
        "    \n",
        "* [1] Goodfellow, I.J., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A. and Bengio, Y. 2014. Generative Adversarial Networks. [arXiv:1406.2661](https://arxiv.org/abs/1406.2661)  [cs, stat].\n",
        "\n",
        "</div>    \n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j3Dbjc2dasiF"
      },
      "source": [
        "<div style=\"background-color: #cce5ff; padding: 10px;\">\n",
        "\n",
        "<h1> Python </h1>\n",
        "\n",
        "Basic python knowledge is assumed for this tutorial. For this tutorial the main machine learning library we'll be working [PyTorch](https://pytorch.org/). Python specific information will be colour coded blue.\n",
        "\n",
        "    \n",
        "## PyTorch\n",
        "    \n",
        "[PyTorch](https://towardsdatascience.com/understanding-pytorch-with-an-example-a-step-by-step-tutorial-81fc5f8c4e8e) is an open source deep learning framework Pytorch is very *pythonic*, so if you are used to coding in python using PyTorch should come naturally.\n",
        "\n",
        "    \n",
        "References:\n",
        "    \n",
        "* [https://towardsdatascience.com/understanding-pytorch-with-an-example-a-step-by-step-tutorial-81fc5f8c4e8e](https://towardsdatascience.com/understanding-pytorch-with-an-example-a-step-by-step-tutorial-81fc5f8c4e8e)\n",
        "* [https://www.nvidia.com/en-us/glossary/data-science/pytorch/](https://www.nvidia.com/en-us/glossary/data-science/pytorch/)    \n",
        "    \n",
        "</div>\n",
        "    \n",
        "<hr>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5gU-dXPVasiF"
      },
      "source": [
        "<div style=\"background-color: #ffffcc; padding: 10px;\">\n",
        "    \n",
        "<h1> Requirements </h1>\n",
        "\n",
        "These notebooks have been tested on on Ubuntu 22.04, MacOS 12 and Windows 2022\n",
        "\n",
        "<h2> Python Packages: </h2>\n",
        "\n",
        "* Python 3\n",
        "* PyTorch\n",
        "* time\n",
        "* os\n",
        "* numpy\n",
        "* matplotlib=3.0\n",
        "\n",
        "\n",
        "<h2> Data Requirements</h2>\n",
        "\n",
        "This notebook referes to some data included in the git hub repositroy\n",
        "    \n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aNSfT4TWasiG"
      },
      "source": [
        "**Contents:**\n",
        "\n",
        "1. [Download MNIST Dataset](#Download-MNIST-Dataset)\n",
        "2. [Building the GAN](#Building-the-GAN)\n",
        "3. [Preparing to train](#Preparing-to-train)\n",
        "4. [Training](#Training)\n",
        "5. [Evaluating the GAN](#Evaluating-the-GAN)\n",
        "6. [Example Application to Earth Sciences](GANS_Tropical_Cyclones_example.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lRVD2-SBasiG"
      },
      "source": [
        "<div style=\"background-color: #cce5ff; padding: 10px;\">\n",
        "\n",
        "**Import python modules**\n",
        "    \n",
        "**PLEASE NOTE MAC USERS:** Torch and Torchvision must be load BEFORE numpy and other libraries due to a Bug in PyToch > 1.1 see [GitHub Issue](https://github.com/pytorch/pytorch/issues/78490). You do not need to change anything below    \n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wNoYx4yaCeGM"
      },
      "outputs": [],
      "source": [
        "# Machine Learning Library PyTorch and specific tools\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.optim import Adam\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import transforms\n",
        "from torchvision.utils import make_grid, save_image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BVBlMQu-asiI"
      },
      "outputs": [],
      "source": [
        "# For readability: disable warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l4U1ikUBasiJ"
      },
      "outputs": [],
      "source": [
        "# import modules\n",
        "# general file system utilites\n",
        "import os\n",
        "import sys\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bZOCy_tkasiK"
      },
      "outputs": [],
      "source": [
        "# Plotting and standard numpy\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xs5_VZ-XasiK"
      },
      "source": [
        "<div style=\"background-color: #cce5ff; padding: 10px;\">\n",
        "\n",
        "Check if GPU's available. This code will run on standard CPU's too just a lots slower.\n",
        "    \n",
        "If no GPU's are detected, then pretrained model weights will be used set by the flag\n",
        "    \n",
        "```python\n",
        "train_local=False\n",
        "```\n",
        "\n",
        "**If you get a message saying no GPU available but you think there should be this means you have not managed to compile a GPU version of PyTorch and need to follow the instructions in [PyTorchCondaRecipe.md](PyTorchCondaRecipe.md)**    \n",
        "\n",
        "or run `conda list cudatoolkit` in your GANS conda evironment and select your installation command from [PyTorch Download Guide](https://pytorch.org/get-started/locally/)\n",
        "    \n",
        "    \n",
        "**You must have NVDIA graphics card for GPU enabled PyTorch**    \n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aTyVAOT7b0LB"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "if torch.cuda.is_available():\n",
        "  train_local=True\n",
        "  GPUs=True\n",
        "  print(torch.cuda.get_device_name(device))\n",
        "  print('GPUs found Setting train_local to True')\n",
        "else:\n",
        "  train_local=False\n",
        "  GPUs=False\n",
        "  print(\"WARNING: No GPU available. Setting train_local to False\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mRpWxv1basiL"
      },
      "source": [
        "<div style=\"background-color: #cce5ff; padding: 10px;\">\n",
        "\n",
        "If you want to override this behaviour and train your own network if you don't mind waiting or if you have GPUS available but don't want to wait 20 mins training your own network later you can the the next cell to:\n",
        "\n",
        "```python\n",
        "override_train_local=True\n",
        "```\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oeAXR2cnasiL"
      },
      "outputs": [],
      "source": [
        "# Set as override_train_local=False to maintain default behaviour for your hardware\n",
        "override_train_local=False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KkNvoWwiasiL"
      },
      "outputs": [],
      "source": [
        "if override_train_local :\n",
        "    if GPUs:\n",
        "        train_local=False\n",
        "        print('WARNING SETTING TRAIN LOCAL TO FALSE, PRETRAINED MODEL WILL BE USED')\n",
        "    else:\n",
        "        train_local=True\n",
        "        print('WARNING SETTING TRAIN LOCAL TO TRUE, WILL TRAIN MODEL OVER A LONG TIME ON CPUS')\n",
        "else:\n",
        "    print('Default behaviour')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SHOnlBoKasiL"
      },
      "source": [
        "<div style=\"background-color: #cce5ff; padding: 10px;\">\n",
        "Create data and image dir\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nyiGZGmFZ3Aq"
      },
      "outputs": [],
      "source": [
        "dataset_path = os.path.join(os.getcwd(), \"data\")\n",
        "image_path = os.path.join(os.getcwd(), \"images\")\n",
        "\n",
        "# Ensure both of the directories are created.\n",
        "if not os.path.exists(dataset_path):\n",
        "    os.mkdir(dataset_path)\n",
        "\n",
        "if not os.path.exists(image_path):\n",
        "    os.mkdir(image_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJB9cyUAZ8b9"
      },
      "source": [
        "# Download MNIST Dataset\n",
        "\n",
        "<div style=\"background-color: #cce5ff; padding: 10px;\">\n",
        "With the file storage out of the way, we can define a few variables. Firstly, we need to set our batch size.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3HcWUgcaaIEG"
      },
      "outputs": [],
      "source": [
        "batch_size = 128"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zUQTC7zUaJK9"
      },
      "source": [
        "<div style=\"background-color: #ccffcc; padding: 10px;\">\n",
        "\n",
        "Next let's create a transform that normalises the pixels in our image to be within the range of -1 to 1. This helps the neural network to train better than leaving the inputs unnormalised.\n",
        "    \n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NIzLaJCXaZSE"
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((.5,), (.5,))])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SWvOzBD2aaJi"
      },
      "source": [
        "<div style=\"background-color: #ccffcc; padding: 10px;\">\n",
        "    \n",
        "We can now load our data. As mentioned above, we're using the MNIST dataset. It's a very simple dataset of 28x28 greyscale images containing handwritten digits from 0 to 9. It has been used as a benchmark since the early days of machine learning, and is simple enough that we can train a GAN on it quite quickly, despite consisting of around 60,000 samples!\n",
        "\n",
        "</div>\n",
        "\n",
        "<div style=\"background-color: #cce5ff; padding: 10px;\">\n",
        "PyTorch contains the MNIST dataset as a baseline, so we can just use their preformed dataset to download the data.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T-vEr9Zaa6dz"
      },
      "outputs": [],
      "source": [
        "mnist_dataset = datasets.MNIST(root=os.path.join(os.getcwd(), \"data\"),\n",
        "                               train=True, download=True, transform=transform)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rpZFbJ1Na922"
      },
      "source": [
        "<div style=\"background-color: #cce5ff; padding: 10px;\">\n",
        "PyTorch uses a DataLoader wrapper around a dataset in order to iterate through the dataset. It's where we specify the number of samples per batch, as well as whether to shuffle the data. drop_last ensures that any batch that isn't at least 128 samples gets dropped.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GR1YsY0ObAtW"
      },
      "outputs": [],
      "source": [
        "mnist_data_loader = DataLoader(mnist_dataset, batch_size=batch_size,\n",
        "                               shuffle=True, drop_last=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IiFzz8XXbCpI"
      },
      "source": [
        "<div style=\"background-color: #ccffcc; padding: 10px;\">\n",
        "Let's confirm how many image samples we have, as well as get a sample of images to show.\n",
        "<div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5lhwVu7gbGAG"
      },
      "outputs": [],
      "source": [
        "print(f'Dataset size: {len(mnist_dataset)} images.')\n",
        "\n",
        "# Get a batch from the dataloader and take 16 images from that batch to display.\n",
        "batch = next(iter(mnist_data_loader))\n",
        "img_batch = batch[0]\n",
        "img_batch_subset = img_batch[:16]  # extract only a subset of images\n",
        "\n",
        "# Form a grid of images and display them.\n",
        "grid = make_grid(img_batch_subset, nrow=int(4), normalize=True, pad_value=1.)\n",
        "\n",
        "# Rearrange axis from channels first to channels last for MatPlotLib.\n",
        "grid = np.moveaxis(grid.numpy(), 0, 2)\n",
        "\n",
        "# Plot our figure.\n",
        "plt.figure(figsize=(6, 6))\n",
        "plt.title(\"MNIST Samples\")\n",
        "plt.imshow(grid)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kiUNHq7GbYum"
      },
      "source": [
        "<div style=\"background-color: #ccffcc; padding: 10px;\">\n",
        "\n",
        "Now we have seen our data, we can start building our neural network!\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hP8sNtyFasiO"
      },
      "source": [
        "# Building the GAN\n",
        "\n",
        "<div style=\"background-color: #ccffcc; padding: 10px;\">\n",
        "Recall that the generator has a latent space from which we draw samples from by input random vectors. Let's define it as a vector with 100 elements and create a function to generate a batch of latent inputs.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RNkp6XM_bgE2"
      },
      "outputs": [],
      "source": [
        "LATENT_SPACE_DIM = 100\n",
        "\n",
        "def get_gaussian_latent_batch(batch_size, device):\n",
        "    return torch.randn((batch_size, LATENT_SPACE_DIM), device=device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-nGiNwkeaeW"
      },
      "source": [
        "\n",
        "\n",
        "<div style=\"background-color: #e6ccff; padding: 10px;\">\n",
        "        \n",
        "We can now start constructing our neural networks. Remember that generative adversarial networks are actually composed of two networks: a generator and a discriminator.\n",
        "\n",
        "    \n",
        "Let's create a building block for constructing the networks first:\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mgDpmkWoenEh"
      },
      "outputs": [],
      "source": [
        "def network_block(in_feat, out_feat, normalize=True, activation=None):\n",
        "    layers = [nn.Linear(in_feat, out_feat)]\n",
        "    if normalize:\n",
        "        layers.append(nn.BatchNorm1d(out_feat))\n",
        "\n",
        "    layers.append(nn.LeakyReLU(0.2) if activation is None else activation)\n",
        "\n",
        "    return layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XuyK8kfGexkD"
      },
      "source": [
        "\n",
        "<div style=\"background-color: #e6ccff; padding: 10px;\">\n",
        "    \n",
        "This building block consists of a Linear (or fully connected) layer, a batch normalisation layer and an activation function layer (Leaky ReLU).\n",
        "\n",
        "</div>    \n",
        "<div style=\"background-color: #ccffcc; padding: 10px;\">\n",
        "Using this block, let's now build our generator, which is tasked with creating new MNIST samples:\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FA9e6XCefF4R"
      },
      "outputs": [],
      "source": [
        "class GeneratorNet(torch.nn.Module):\n",
        "    def __init__(self, img_shape=(28, 28)):\n",
        "        super().__init__()\n",
        "        self.generated_img_shape = img_shape\n",
        "        num_neurons_per_layer = [LATENT_SPACE_DIM, 256, 512, 1024, img_shape[0] * img_shape[1]]\n",
        "\n",
        "        # Let's define our network as a series of four of the blocks we defined above. Each time we increase the\n",
        "        # number of neurons per layer and use the leaky ReLU as our activation function. For the last set of layers,\n",
        "        # we skip normalisation and instead apply a hyperbolic tangent as our activation layer. This is so that the\n",
        "        # output of the network is in the range of -1 to 1, with the same number of pixels as the inputs.\n",
        "        self.net = nn.Sequential(\n",
        "            *network_block(num_neurons_per_layer[0], num_neurons_per_layer[1]),\n",
        "            *network_block(num_neurons_per_layer[1], num_neurons_per_layer[2]),\n",
        "            *network_block(num_neurons_per_layer[2], num_neurons_per_layer[3]),\n",
        "            *network_block(num_neurons_per_layer[3], num_neurons_per_layer[4], normalize=False, activation=nn.Tanh())\n",
        "        )\n",
        "\n",
        "    def forward(self, latent_vector_batch):\n",
        "        img_batch_flattened = self.net(latent_vector_batch)\n",
        "\n",
        "        # The output of the network is a vector of 784 pixels, let's reshape it so it's a matrix of 28x28 pixels.\n",
        "        return img_batch_flattened.view(img_batch_flattened.shape[0], 1, *self.generated_img_shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EGj3vOkFfLnr"
      },
      "source": [
        "\n",
        "<div style=\"background-color: #e6ccff; padding: 10px;\">\n",
        "    \n",
        "The above generator is built using four of the network blocks we defined above, each time increase the number of neurons per layer. The final block uses a hyperbolic tangent as its activation function, as this allows us to shape the output in the range of -1 to 1, which is the range our pixels are stored in.\n",
        "\n",
        "    \n",
        "Next, we'll define the discriminator in a similar manner:\n",
        "</div>    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q5E5RpyifiG_"
      },
      "outputs": [],
      "source": [
        "class DiscriminatorNet(torch.nn.Module):\n",
        "    def __init__(self, img_shape=(28, 28)):\n",
        "        super().__init__()\n",
        "        num_neurons_per_layer = [img_shape[0] * img_shape[1], 512, 256, 1]\n",
        "\n",
        "        # Last layer is Sigmoid function - basically the goal of the discriminator is to output 1.\n",
        "        # for real images and 0. for fake images and sigmoid is clamped between 0 and 1 so it's perfect.\n",
        "        self.net = nn.Sequential(\n",
        "            *network_block(num_neurons_per_layer[0], num_neurons_per_layer[1], normalize=False),\n",
        "            *network_block(num_neurons_per_layer[1], num_neurons_per_layer[2], normalize=False),\n",
        "            *network_block(num_neurons_per_layer[2], num_neurons_per_layer[3], normalize=False, activation=nn.Sigmoid())\n",
        "        )\n",
        "\n",
        "    def forward(self, img_batch):\n",
        "        # We remove the redundant channel dimension here, since it's of size 1 (we're dealing with greyscale images\n",
        "        # remember).\n",
        "        img_batch_flattened = img_batch.view(img_batch.shape[0], -1)\n",
        "        return self.net(img_batch_flattened)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q9SpP12_fkzO"
      },
      "source": [
        "\n",
        "<div style=\"background-color: #e6ccff; padding: 10px;\">\n",
        "    \n",
        "We use the same block we created for the generator to keep things simple, and just reverse the process: going from an image of 28x28 to a single floating point number representing the certainty that the discriminator has of a sample being real or generated.\n",
        "\n",
        "Now we've defined our generator and discriminator, all that remains is to create them!\n",
        "    <div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MM_DkXcTfxZw"
      },
      "outputs": [],
      "source": [
        "# We send both networks to our training device\n",
        "discriminator_net = DiscriminatorNet().to(device)\n",
        "generator_net = GeneratorNet().to(device)\n",
        "\n",
        "# Make sure they're both set to train!\n",
        "discriminator_net.train()\n",
        "generator_net.train()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iI7u7aOsf_5P"
      },
      "source": [
        "# Preparing to train\n",
        "\n",
        "<div style=\"background-color: #e6ccff; padding: 10px;\">\n",
        "    \n",
        "Now we've created our generator and discriminator, we need to train them! As it stands, they've been initiated with random weights for all trainable parameters. This means that initially, both the generator and discriminator are going to be terrible at their respective tasks.\n",
        "\n",
        "By training them, both will improve, and through the adversarial nature of the loss function, one network getting stronger *should* result in the other improving too. This isn't always the case however, sometimes either the generator or discriminator will become too strong and overpower the other. This is a failure case, and the model will no longer produce anything meaningful.\n",
        "\n",
        "In the case of the generator winning, it learns a pattern that fools the discriminator while not necessarily representing the dataset at all. Often, the mode will collpase, with all latent variables converging to the same output image. If the discriminator wins, the generator loses its gradient and will begin to produce garbage outputs.\n",
        "\n",
        "We want to avoid both cases, which makes GANs more difficult to train than most other machine learning models.\n",
        "\n",
        "To begin our training, we first need to create an optimiser for each network. This optimiser implements an algorithm for performing backpropagation on the neural network. For this demo, we'll use the well known and effective Adam optimiser. If you want to know more about how this works, the paper it was proposed in can be found [here](https://arxiv.org/abs/1412.6980).\n",
        "    \n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yvPnZloFasiQ"
      },
      "source": [
        "# Learning Rates"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jML-BrmehiiU"
      },
      "source": [
        "<div style=\"background-color: #e6ccff; padding: 10px;\">\n",
        "\n",
        "We set the initial learning rate for both optimisers to 0.0002. The learning rate is the single most important hyperparameter when training, and changing it can have drastic results. Try experimenting with much larger learning rates on both the generator and discriminator and see how it affects training!\n",
        "\n",
        "</div>\n",
        "\n",
        "<div style=\"background-color: #cce5ff; padding: 10px;\">\n",
        "\n",
        "    \n",
        "**There is a link back to here at the end of this section of the tutorial.**\n",
        "    \n",
        "After you have gone through the tutorial rerun the section below with different learning rates to see the effects.\n",
        "    \n",
        "The learning rates are initally set at:\n",
        "```python\n",
        "    learning_rate_dis = 0.0002\n",
        "    learning_rate_gen = 0.0002\n",
        "```    \n",
        "    \n",
        "For example try increasing or decreasing by a factor of 10. What happens if you alter one rate but not the other?   \n",
        "    \n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ukymBO6KasiQ"
      },
      "outputs": [],
      "source": [
        "# Learning Rate\n",
        "# We set the initial learning rate for both optimisers to 0.0002.\n",
        "learning_rate_dis = 0.00002\n",
        "learning_rate_gen = 0.00002"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NMv678urgo2F"
      },
      "outputs": [],
      "source": [
        "discriminator_opt = Adam(discriminator_net.parameters(), lr=learning_rate_dis)\n",
        "generator_opt = Adam(generator_net.parameters(), lr=learning_rate_gen)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YmIDVX9LasiR"
      },
      "source": [
        "<div style=\"background-color: #ccffcc; padding: 10px;\">\n",
        "In order to evaluate how the network is doing, we'll use a constant set of 16 latent inputs and monitor how the outputs change, saving a sample to the images directory.\n",
        "    </div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y8JFUtLPiAQ-"
      },
      "outputs": [],
      "source": [
        "ref_batch_size = 16\n",
        "ref_noise_batch = get_gaussian_latent_batch(ref_batch_size, device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uq9AwRIqiEV9"
      },
      "source": [
        "\n",
        "<div style=\"background-color: #e6ccff; padding: 10px;\">\n",
        "    \n",
        "We need to define the loss function that we're going to use to train our networks. We'll use binary cross entropy to measure our discriminator's predictions.\n",
        "\n",
        "If we input real images into the discriminator we expect it to output 1 (100% certainty that the image is real).\n",
        "\n",
        "The further away it is from 1 and the closer it is to 0 the more we should penalize it, as it is making an incorrect prediction.\n",
        "\n",
        "BCE loss essentially becomes -log(x) when the target label is 1.\n",
        "\n",
        "Similarly for fake images, the target label is 0 (as we want the discriminator to output 0 for fake images) and we want to penalise the generator if it starts outputing values close to 1. So we basically want to mirror the above loss function and that's just: -log(1-x).\n",
        "\n",
        "BCE loss basically becomes -log(1-x) when it's target label is 0.\n",
        "    \n",
        "</div>    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "taXv-ChNi3zJ"
      },
      "outputs": [],
      "source": [
        "adversarial_loss = nn.BCELoss()\n",
        "\n",
        "# Let's define our ground truth for each type of input.\n",
        "real_images_gt = torch.ones((batch_size, 1), device=device)\n",
        "fake_images_gt = torch.zeros((batch_size, 1), device=device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u04qLbP9i_MP"
      },
      "source": [
        "\n",
        "<div style=\"background-color: #ccffcc; padding: 10px;\">\n",
        "Finally, let's define a few variables to help our training process.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FFDe16zdjDtj"
      },
      "outputs": [],
      "source": [
        "# Let's create some variables to keep track of our losses, as well as configure how often we log.\n",
        "discriminator_loss_values = []\n",
        "generator_loss_values = []\n",
        "img_count = 0\n",
        "console_log_freq = 50\n",
        "debug_imagery_log_freq = 50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZwjRIm2gasiS"
      },
      "outputs": [],
      "source": [
        "# This defines how many times we show the entire dataset to each network. 100 epochs means we show the networks every\n",
        "# sample 100 times. Feel free to experiment with this and see if it affects training!\n",
        "num_epochs = 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SIcRNuZoasiS"
      },
      "outputs": [],
      "source": [
        "# Let's also measure how much time training takes.\n",
        "start_time = time.time()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33nD90rijMSo"
      },
      "source": [
        "<div style=\"background-color: #ccffcc; padding: 10px;\">\n",
        "We're now ready to begin our training loop!\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWBaUcgtasiS"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8z-21fNDjWyI"
      },
      "source": [
        "<div style=\"background-color: #ccffcc; padding: 10px;\">\n",
        "Let's create our GAN training loop. We start with training the discriminator, as this helps to prevent early mode collapse.\n",
        "</div>   \n",
        "\n",
        "<div style=\"background-color: #ffcdcc; padding: 10px;\">     \n",
        "\n",
        " **For 100 epochs, this training will take approximately 20-25 minutes, go grab a coffee!**\n",
        "\n",
        "</div>    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jj_frJj4jeFX"
      },
      "outputs": [],
      "source": [
        "if train_local:\n",
        "    for epoch in range(num_epochs):\n",
        "        for batch_idx, (real_images, _) in enumerate(mnist_data_loader):\n",
        "\n",
        "            # Move the batch tensor to our training device.\n",
        "            real_images = real_images.to(device)\n",
        "\n",
        "            # Zero out the gradients in the discriminator. It's essential to do this prior to performing a forward and\n",
        "            # backwards pass, else we have stale gradients from the previous iteration still present.\n",
        "            discriminator_opt.zero_grad()\n",
        "\n",
        "            # Show real images to our discriminator.\n",
        "            real_discriminator_loss = adversarial_loss(discriminator_net(real_images), real_images_gt)\n",
        "\n",
        "            # Generate fake images with the generator.\n",
        "            fake_images = generator_net(get_gaussian_latent_batch(batch_size, device))\n",
        "\n",
        "            # Show fake images to our discriminator. We call .detach() on them to detach them from the gradient graph\n",
        "            # since we're not training the generator right now.\n",
        "            fake_discriminator_loss = adversarial_loss(discriminator_net(fake_images.detach()), fake_images_gt)\n",
        "\n",
        "            # Calculate the final loss by adding both terms and performing a backpropagation pass and optimiser step.\n",
        "            discriminator_loss = real_discriminator_loss + fake_discriminator_loss\n",
        "            discriminator_loss.backward()\n",
        "            discriminator_opt.step()\n",
        "\n",
        "            # Zero out the gradients in the generator.\n",
        "            generator_opt.zero_grad()\n",
        "\n",
        "            # Generate fake images and show them to the discriminator.\n",
        "            generated_images_predictions = discriminator_net(generator_net(get_gaussian_latent_batch(batch_size, device)))\n",
        "\n",
        "            # The generator's goal is to fool the discriminator, so our loss function should aim to make the\n",
        "            # discriminator classify these fake images as real.\n",
        "            generator_loss = adversarial_loss(generated_images_predictions, real_images_gt)\n",
        "\n",
        "            # Perform a backpropagation pass and optimiser step.\n",
        "            generator_loss.backward()  # this will populate .grad vars in the G net (also in D but we won't use those)\n",
        "            generator_opt.step()  # perform G weights update according to optimizer's strategy\n",
        "\n",
        "            # Log our loss values. We detach them from the graph to ensure we don't get a memory leak.\n",
        "            generator_loss_values.append(generator_loss.detach().item())\n",
        "            discriminator_loss_values.append(discriminator_loss.detach().item())\n",
        "\n",
        "            # Log details to the console.\n",
        "            if batch_idx % console_log_freq == 0:\n",
        "                prefix = 'GAN training: time elapsed'\n",
        "                print(f\"{prefix} = {(time.time() - start_time):.2f} [s] | epoch={epoch + 1} | \"\n",
        "                      f\"batch= [{batch_idx + 1}/{len(mnist_data_loader)}]\")\n",
        "\n",
        "            # Save intermediate generator images.\n",
        "            if batch_idx % debug_imagery_log_freq == 0:\n",
        "                with torch.no_grad():\n",
        "                    log_generated_images = generator_net(ref_noise_batch)\n",
        "                    log_generated_images_resized = nn.Upsample(scale_factor=2.5, mode='nearest')(log_generated_images)\n",
        "                    out_path = os.path.join(image_path, f'{str(img_count).zfill(6)}.jpg')\n",
        "                    save_image(log_generated_images_resized, out_path, nrow=int(np.sqrt(ref_batch_size)), normalize=True)\n",
        "                    img_count += 1\n",
        "    # Trained\n",
        "    trained = True\n",
        "else:\n",
        "    print('Train_local set to False loading pretrained')\n",
        "    try:\n",
        "        generator_net = GeneratorNet()\n",
        "        generator_net.load_state_dict(torch.load('pretrained/GANS_GenN_gen_LR_' + str(learning_rate_gen) +\n",
        "                                                 '_dis_LR_' + str(learning_rate_dis) + '.pt',map_location=torch.device('cpu')))\n",
        "        discriminator_net = DiscriminatorNet()\n",
        "        discriminator_net.load_state_dict(torch.load('pretrained/GANS_DisN_gen_LR_' + str(learning_rate_gen) +\n",
        "                                                 '_dis_LR_' + str(learning_rate_dis) + '.pt',map_location=torch.device('cpu')))\n",
        "    except FileNotFoundError:\n",
        "        print('pretrained/GANS_DisN_gen_LR_' + str(learning_rate_gen) + '_dis_LR_' + str(learning_rate_dis) +\n",
        "              '.pkl not found,try using the following learning rates 0.002, 0.0002 or 0.00002.')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "\n",
        "mkdir pretrained"
      ],
      "metadata": {
        "id": "cYOzlps9FFhx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Oq69ZvdasiT"
      },
      "outputs": [],
      "source": [
        "if train_local:\n",
        "    if trained:\n",
        "        torch.save(generator_net.state_dict(), 'pretrained/GANS_GenN_gen_LR_' + str(learning_rate_gen) +\n",
        "                                                 '_dis_LR_' + str(learning_rate_dis) + '.pt')\n",
        "        torch.save(discriminator_net.state_dict(), 'pretrained/GANS_DisN_gen_LR_' + str(learning_rate_gen) +\n",
        "                                                 '_dis_LR_' + str(learning_rate_dis) + '.pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91acZT2CjvhX"
      },
      "source": [
        "<div style=\"background-color: #ccffcc; padding: 10px;\">\n",
        "\n",
        "Now our training is finished, it's time to evaluate how it's doing!\n",
        "\n",
        "If you'd like, you can check out the image samples we saved during training. Remember we created a set of constant latent inputs, meaning each image has the same input and differs only because the generator's weights have updated.\n",
        "\n",
        "The images folder contains samples produced as training has progressed. Can you see how it's improving over time?\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WftIQ-UOasiT"
      },
      "source": [
        "# Evaluating the GAN\n",
        "\n",
        "<div style=\"background-color: #ccffcc; padding: 10px;\">\n",
        "\n",
        "Now we've trained our model, we can use it to generate a new handwritten digit based on the MNIST dataset, but totally unique!\n",
        "\n",
        "To start with, we'll define a few functions for prettying up and displaying the sample images.\n",
        "    \n",
        "</div>    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V_LYQv1qmFbQ"
      },
      "outputs": [],
      "source": [
        "def postprocess_generated_img(generated_img_tensor):\n",
        "    assert isinstance(generated_img_tensor, torch.Tensor), f'Expected PyTorch tensor but got {type(generated_img_tensor)}.'\n",
        "\n",
        "    # Move the tensor from GPU to CPU, convert to numpy array, extract 0th batch, move the image channel\n",
        "    # from 0th to 2nd position (CHW -> HWC)\n",
        "    generated_img = np.moveaxis(generated_img_tensor.to('cpu').numpy()[0], 0, 2)\n",
        "\n",
        "    # Since MNIST images are grayscale (1-channel only) repeat 3 times to get RGB image\n",
        "    generated_img = np.repeat(generated_img,  3, axis=2)\n",
        "\n",
        "    # Normalise from range of [-1, 1], to [0, 1].\n",
        "    generated_img -= np.min(generated_img)\n",
        "    generated_img /= np.max(generated_img)\n",
        "\n",
        "    return generated_img\n",
        "\n",
        "\n",
        "# This function will generate a random vector pass it to the generator which will generate a new image\n",
        "# which we will just post-process and return it\n",
        "def generate_from_random_latent_vector(generator):\n",
        "    with torch.no_grad():  # Tells PyTorch not to compute gradients which would have huge memory footprint\n",
        "\n",
        "        # Generate a single random (latent) vector\n",
        "        latent_vector = get_gaussian_latent_batch(1, next(generator.parameters()).device)\n",
        "\n",
        "        # Post process generator output (as it's in the [-1, 1] range, remember?)\n",
        "        generated_img = postprocess_generated_img(generator(latent_vector))\n",
        "\n",
        "    return generated_img\n",
        "\n",
        "\n",
        "def display_image(numpy_image):\n",
        "    assert isinstance(numpy_image, np.ndarray), f'Expected numpy array got {type(numpy_image)}.'\n",
        "\n",
        "    # Convert to uint8 format if it isn't already.\n",
        "    if numpy_image.dtype != np.uint8:\n",
        "        numpy_image = (numpy_image*255).astype(np.uint8)\n",
        "\n",
        "    # Show the image.\n",
        "    plt.imshow(numpy_image)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oEYqXBRknKzz"
      },
      "source": [
        "<div style=\"background-color: #cce5ff; padding: 10px;\">\n",
        "Now let's generate a sample image from our generator! First we need to set the generator to evaluation mode, to prevent PyTorch from generating gradients.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-HSP7csznZsY"
      },
      "outputs": [],
      "source": [
        "generator_net.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OuZXzuohnbwu"
      },
      "source": [
        "<div style=\"background-color: #ccffcc; padding: 10px;\">\n",
        "Finally, let's generate our new sample!\n",
        "</div>    \n",
        "\n",
        "<div style=\"background-color: #cce5ff; padding: 10px;\">\n",
        "    \n",
        "**You can rerun the below cell multiple times to generate new, unique samples.**\n",
        "    \n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T7AWo0ianfph"
      },
      "outputs": [],
      "source": [
        "generated_img = generate_from_random_latent_vector(generator_net)\n",
        "display_image(generated_img)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tKrM8J5EniQ7"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "<div style=\"background-color: #ccffcc; padding: 10px;\">    \n",
        "\n",
        "**Now is the time to go back and test the effect of latering those [Learning Rates](#Learning-Rates)**\n",
        "    \n",
        "Please feel free to experiment with the network architectures or hyperparameters. You could also try and intentionally cause the generator or discriminator to win and see what happens to the output.\n",
        "\n",
        "For any questions about using GANs, or how they could be applied to your own research problems please contact Caitlin Howarth at sccmho@leeds.ac.uk.\n",
        "</div>    \n",
        "\n",
        "<hr>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ixP0jpT5asiV"
      },
      "source": [
        "<hr>\n",
        "\n",
        "\n",
        "<div style=\"background-color: #e6ccff; padding: 10px;\">\n",
        "    \n",
        "<h1> Earth Science Applications for GANS </h1>\n",
        "    \n",
        "Hopefully in the future we'll add an Earth Science specific tutorial to add but hopefully you can take the information you've learnt from this tutorial to underdtand these Earth Science applications.\n",
        "    \n",
        "# Met Office using GANs to predict Tropical Cyclones\n",
        "    \n",
        "The [MetOffice](https://github.com/MetOffice/) have a Git repo example [ML-TC](https://github.com/MetOffice/ML-TC), which takes open souce [Bangladesh Tropical Cyclone Data](https://doi.org/10.1038/s41597-021-00847-5) to train a Generative Adviserial Network to output synthetic tropical cyclone data. *WARNING if you chose to run these scripts the dataset download is over **100GB**. This is not suitable for running on your home laptop*     \n",
        "    \n",
        "# IBM using GANs for digitizing Earth Surface Sketches    \n",
        "    \n",
        "IMB have a short article and video using GANS to generate realistic synthetic seismic images from hand sketches. Read the article here: [Generating Sketch-Based Realistic Seismic Images with Generative Adversarial Networks, Rodrigo S. Ferreira, Julia Noce, Dario A. B. Oliveira and Emilio Vital Brazil](https://www.researchgate.net/publication/336573727_Generating_Sketch-Based_Synthetic_Seismic_Images_With_Generative_Adversarial_Networks).\n",
        "    \n",
        "Or view the video summary taken from the [IMB blog](https://www.ibm.com/blogs/research/2019/06/good-gans/) by running the cell below:\n",
        "\n",
        "</div>    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VUXMDZZCasiV"
      },
      "outputs": [],
      "source": [
        "%%html\n",
        "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/n69377yKd50\" title=\"GANs for Good: Digitizing Sketches of the Earth’s Surface with the AI Behind Deep Fakes\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dXbylIaFasiV"
      },
      "source": [
        "<div style=\"background-color: #ccffcc; padding: 10px;\">\n",
        "    \n",
        "Finally, if you have a good tutorial sized Earth Science aplication using GANs that you would like to share please get in touch via the [GitHub Repo](https://github.com/cemac/LIFD_GenerativeAdversarialNetworks).     \n",
        "    \n",
        "</div>      \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2d5XNPKmasiV"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bH0_nKlmasiV"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Copy of gan_intro.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}